# Stats-concepts
general stats concepts

AIC: Akaikeâ€™s Information Criterion
https://www.statisticshowto.datasciencecentral.com/akaikes-information-criterion/

Log-likelihood: the natural logarithm of the likelihood
https://www.statlect.com/glossary/log-likelihood#targetText=In%20turn%2C%20given%20a%20sample,Definition

Marginal Models: a type of linear model that accounts for repeated response measures on the same subject
Five Extensions of the General Linear Model: https://www.theanalysisfactor.com/extensions-general-linear-model/#targetText=Marginal%20models%20are%20a%20type,observations%20of%20a%20single%20subject.

Robust Standard Errors: a technique to obtain unbiased standard errors of OLS coefficients under heteroscedasticity
https://economictheoryblog.com/2016/08/07/robust-standard-errors/#targetText=%E2%80%9CRobust%E2%80%9D%20standard%20errors%20is%20a,of%20OLS%20coefficients%20under%20heteroscedasticity.&targetText=%E2%80%9CRobust%E2%80%9D%20standard%20errors%20have%20many,the%20sandwich%20estimator%20of%20variance.

Bonferonni correction: divide the alpha value by the number of tests

Holm-Bonferroni Method: It is a modification of the Bonferroni correction. The Bonferroni correction reduces the possibility of 
getting a statistically significant result (i.e. a Type I error) when performing multiple tests. Although the Bonferroni is 
simple to calculate, it suffers from a lack of statistical power. The Holm-Bonferroni method is also fairly simple to 
calculate, but it is more powerful than the single-step Bonferroni. https://www.statisticshowto.datasciencecentral.com/holm-bonferroni-method/

Block randomization: designed to randomize subjects into groups that result in equal sample sizes. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3136079/

Cox proportional hazards regression: survival analysis http://www.sthda.com/english/wiki/cox-proportional-hazards-model

Standard deviation of the residuals: https://www.khanacademy.org/math/ap-statistics/bivariate-data-ap/assessing-fit-least-squares-regression/v/standard-dev-residuals

Standard deviation vs. standard error of the mean: standard error of the mean is the measure of how far your sample mean is likely to be from the true mean of the population. So the lower your SEM is, the more likely it is that your calculated mean is close to the actual mean. Estimate SEM: SEM = SD / sqrt(n).
SD is a measure of the variability of the data, and SEM is a measure of precision of the data.
https://www.youtube.com/watch?v=3UPYpOLeRJg

Why Overlapping Confidence Intervals mean Nothing about Statistical Significance:
https://towardsdatascience.com/why-overlapping-confidence-intervals-mean-nothing-about-statistical-significance-48360559900a

Maximum likelihood estimation: 

Likelihood function: 

Permutation: https://medium.com/i-math/combinations-permutations-fa7ac680f0ac
